{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "381ea721f2ab75d8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def one_qubit_rotation(qc, weights, gate_types_per_layer, enable_rx=True, enable_ry=True, enable_rz=True):\n",
    "    \"\"\"\n",
    "    Adds rotation gates around the X, Y, and Z axis to the quantum circuit for a specific qubit,\n",
    "    with the rotation angles specified by the values in `weights`.\n",
    "    \"\"\"\n",
    "    if len(weights) != gate_types_per_layer * len(qc.qubits):\n",
    "        print(f\"len weights {len(weights)}\")\n",
    "        print(f\"gate types per layer {gate_types_per_layer}\")\n",
    "        print(f\"len qubits {len(qc.qubits)}\")\n",
    "        print(f\"len qubits * gate types per layer {len(qc.qubits) * gate_types_per_layer}\")\n",
    "        raise ValueError(\n",
    "            \"The number of weights must be equal to the number of qubits times the number of gate types per layer\")\n",
    "\n",
    "    if not enable_rx and not enable_ry and not enable_rz:\n",
    "        raise ValueError(\"At least one gate must be enabled\")\n",
    "\n",
    "    enabled_gates = sum([enable_rx, enable_ry, enable_rz])  # Count how many gates are enabled\n",
    "    if len(weights) != enabled_gates * len(qc.qubits):\n",
    "        print(len(weights))\n",
    "        print(enabled_gates)\n",
    "        print(len(qc.qubits))\n",
    "        print(len(qc.qubits) * enabled_gates)\n",
    "        raise ValueError(\"The number of weights does not match the number of enabled gates times the number of qubits\")\n",
    "\n",
    "    for qubit in qc.qubits:\n",
    "        index = qubit.index\n",
    "        weight_index = 0  # Initialize weight index for each qubit\n",
    "\n",
    "        if enable_rx:\n",
    "            qc.rx(weights[index + weight_index * len(qc.qubits)], index)  # Rotate around X-axis\n",
    "            weight_index += 1  # Move to the next set of weights\n",
    "\n",
    "        if enable_ry:\n",
    "            qc.ry(weights[index + weight_index * len(qc.qubits)], index)  # Rotate around Y-axis\n",
    "            weight_index += 1  # Move to the next set of weights\n",
    "\n",
    "        if enable_rz:\n",
    "            qc.rz(weights[index + weight_index * len(qc.qubits)], index)  # Rotate around Z-axis\n",
    "            # No need to increase weight_index here since it's the last operation\n",
    "\n",
    "\n",
    "def entangling_layer(qc: QuantumCircuit):\n",
    "    \"\"\"\n",
    "    Adds a layer of CZ entangling gates (controlled-Z) on `qubits` (arranged in a circular topology) to the quantum circuit.\n",
    "    \"\"\"\n",
    "    # Assume `qc` is your QuantumCircuit object that's defined outside this function\n",
    "    for i in range(qc.num_qubits - 1):\n",
    "        qc.cz(i, i + 1)  # Apply CZ between consecutive qubits\n",
    "    if qc.num_qubits > 2:  # If more than 2 qubits, connect the first and last qubits to form a circle\n",
    "        qc.cz(0, qc.num_qubits - 1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1214fc45c9c32d67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "def generate_circuit(qc, n_layers, enable_rx=True, enable_ry=True, enable_rz=True):\n",
    "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
    "    # Number of qubits\n",
    "    n_qubits = qc.num_qubits\n",
    "    # if gate is enabled, add a parameter for each qubit and for each layer\n",
    "\n",
    "    gate_types_per_layer = 0\n",
    "    if enable_rx:\n",
    "        gate_types_per_layer += 1\n",
    "    if enable_ry:\n",
    "        gate_types_per_layer += 1\n",
    "    if enable_rz:\n",
    "        gate_types_per_layer += 1\n",
    "    if gate_types_per_layer == 0:\n",
    "        raise ValueError(\"At least one gate must be enabled\")\n",
    "    if n_layers < 1:\n",
    "        raise ValueError(\"At least one layer is required\")\n",
    "    if n_qubits < 1:\n",
    "        raise ValueError(\"At least one qubit is required\")\n",
    "\n",
    "    params = ParameterVector(\"theta\", gate_types_per_layer * (n_layers + 1) * n_qubits)\n",
    "    inputs = ParameterVector(\"inputs\", n_qubits)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        for j in range(n_qubits):\n",
    "            qc.rx(inputs[j], j)\n",
    "        qc.barrier()\n",
    "        # Variational layer\n",
    "        if i == 0:\n",
    "            one_qubit_rotation(qc, params[0:gate_types_per_layer * n_qubits], gate_types_per_layer=gate_types_per_layer,\n",
    "                               enable_rx=enable_rx, enable_ry=enable_ry, enable_rz=enable_rz)\n",
    "        else:\n",
    "            one_qubit_rotation(qc,\n",
    "                               params[i * gate_types_per_layer * n_qubits:(i + 1) * gate_types_per_layer * n_qubits],\n",
    "                               gate_types_per_layer=gate_types_per_layer, enable_rx=enable_rx, enable_ry=enable_ry,\n",
    "                               enable_rz=enable_rz)\n",
    "        qc.barrier()\n",
    "        entangling_layer(qc)\n",
    "        # Encoding layer\n",
    "        qc.barrier()\n",
    "\n",
    "    for i in range(n_qubits):\n",
    "        qc.rx(inputs[i], i)\n",
    "    one_qubit_rotation(qc, params[\n",
    "                           n_layers * gate_types_per_layer * n_qubits:(n_layers + 1) * gate_types_per_layer * n_qubits],\n",
    "                       gate_types_per_layer=gate_types_per_layer, enable_rx=enable_rx, enable_ry=enable_ry,\n",
    "                       enable_rz=enable_rz)\n",
    "\n",
    "    return qc, list(params), list(inputs)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a35d17fbee8d4fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "qc = QuantumCircuit(4)\n",
    "qc, params, inputs = generate_circuit(qc, 8, enable_rx=True, enable_ry=True, enable_rz=True)\n",
    "qc.draw(\"mpl\", style=\"iqx\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "250f0cb41527ed45"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sampler_qnn = SamplerQNN(circuit=qc, input_params=inputs, weight_params=params)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b835266fdcf8fa91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qnn = TorchConnector(sampler_qnn)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9af0bde9ba025557"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mqnn\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'qnn' is not defined"
     ]
    }
   ],
   "source": [
    "qnn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T21:10:37.231355324Z",
     "start_time": "2024-03-01T21:10:37.051438103Z"
    }
   },
   "id": "1ab2b62adac70dce",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class ReUploadingPQC(nn.Module):\n",
    "    \"\"\"\n",
    "    Placeholder PyTorch implementation. Note that quantum circuit generation and ControlledPQC are not\n",
    "    directly implemented, as they depend on the quantum framework used with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, qc, n_layers, observables, activation='linear', name=\"re-uploading_PQC\"):\n",
    "        super(ReUploadingPQC, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qc.num_qubits)\n",
    "        \n",
    "        # Placeholder for circuit generation\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qc, n_layers) # Needs implementation\n",
    "        \n",
    "        self.theta = nn.Parameter(\n",
    "            torch.rand(1, len(theta_symbols)) * np.pi,  # Uniform initialization [0, pi]\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "        self.lmbd = nn.Parameter(\n",
    "            torch.ones(self.n_qubits * self.n_layers),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "        # Placeholder for defining symbol order and computation layer\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = torch.tensor([symbols.index(a) for a in sorted(symbols)])\n",
    "        \n",
    "        if activation == 'linear':\n",
    "            self.activation = nn.Identity()\n",
    "        else:\n",
    "            # Add more activation functions as needed\n",
    "            self.activation = getattr(nn, activation.capitalize())()\n",
    "        \n",
    "        # Placeholder for ControlledPQC and empty circuit\n",
    "        self.computation_layer = None  # Needs implementation\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_dim = inputs[0].shape[0]\n",
    "        # Placeholder for repeating operations related to quantum circuits\n",
    "        tiled_up_thetas = self.theta.repeat(batch_dim, 1)\n",
    "        tiled_up_inputs = inputs[0].repeat(1, self.n_layers)\n",
    "        scaled_inputs = tiled_up_inputs * self.lmbd\n",
    "        \n",
    "        squashed_inputs = self.activation(scaled_inputs)\n",
    "        \n",
    "        joined_vars = torch.cat([tiled_up_thetas, squashed_inputs], dim=1)\n",
    "        joined_vars = joined_vars[:, self.indices]\n",
    "        \n",
    "        # Placeholder for computation with the quantum layer\n",
    "        # return self.computation_layer([empty_circuits, joined_vars])\n",
    "        return joined_vars  # Placeholder return\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b614fe1ce8d671a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class encoding_layer(torch.nn.Module):\n",
    "    def __init__(self, num_qubits=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define weights for the layer\n",
    "        weights = torch.Tensor(num_qubits)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, -1, 1)  # <--  Initialization strategy\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step, as explained above.\"\"\"\n",
    "\n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x)\n",
    "\n",
    "        x = self.weights * x\n",
    "        x = torch.atan(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class exp_val_layer(torch.nn.Module):\n",
    "    def __init__(self, action_space=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the weights for the layer\n",
    "        weights = torch.Tensor(action_space)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, 35, 40)  # <-- Initialization strategy (heuristic choice)\n",
    "\n",
    "        # Masks that map the vector of probabilities to <Z_0*Z_1> and <Z_2*Z_3>\n",
    "        self.mask_ZZ_12 = torch.tensor([1., -1., -1., 1., 1., -1., -1., 1., 1., -1., -1., 1., 1., -1., -1., 1.],\n",
    "                                       requires_grad=False)\n",
    "        self.mask_ZZ_34 = torch.tensor([-1., -1., -1., -1., 1., 1., 1., 1., -1., -1., -1., -1., 1., 1., 1., 1.],\n",
    "                                       requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step, as described above.\"\"\"\n",
    "\n",
    "        expval_ZZ_12 = self.mask_ZZ_12 * x\n",
    "        expval_ZZ_34 = self.mask_ZZ_34 * x\n",
    "\n",
    "        # Single sample\n",
    "        if len(x.shape) == 1:\n",
    "            expval_ZZ_12 = torch.sum(expval_ZZ_12)\n",
    "            expval_ZZ_34 = torch.sum(expval_ZZ_34)\n",
    "            out = torch.cat((expval_ZZ_12.unsqueeze(0), expval_ZZ_34.unsqueeze(0)))\n",
    "\n",
    "        # Batch of samples\n",
    "        else:\n",
    "            expval_ZZ_12 = torch.sum(expval_ZZ_12, dim=1, keepdim=True)\n",
    "            expval_ZZ_34 = torch.sum(expval_ZZ_34, dim=1, keepdim=True)\n",
    "            out = torch.cat((expval_ZZ_12, expval_ZZ_34), 1)\n",
    "\n",
    "        return self.weights * ((out + 1.) / 2.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
